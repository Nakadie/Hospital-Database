{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea662a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing mario dependencies\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import save_model\n",
    "from keras.models import load_model\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80e3b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "done = True\n",
    "\n",
    "for step in range(100000):\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    print(info)\n",
    "    total_reward += reward\n",
    "    clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c2ffdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Create variables for agent\n",
    "        self.state_space = state_size\n",
    "        self.action_space = action_size\n",
    "        self.memory = deque(maxlen = 5000)\n",
    "        self.gamma = 0.8\n",
    "        self.chosenAction = 0\n",
    "        \n",
    "        # Exploration vs explotation\n",
    "        self.epsilon = 1\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay_epsilon = 0.0001\n",
    "        \n",
    "        # Building Neural Networks for Agent\n",
    "        self.main_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (4,4), strides=4, padding='same', input_shape=self.state_space))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (4,4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (3,3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())  \n",
    "        \n",
    "    def act(self, state, onGround):\n",
    "        if onGround < 83:\n",
    "            print('On Ground')\n",
    "            if random.uniform(0,1) < self.epsilon:\n",
    "                self.chosenAction = np.random.randint(self.action_space)\n",
    "                return self.chosenAction\n",
    "            Q_value = self.main_network.predict(state)\n",
    "            self.chosenAction = np.argmax(Q_value[0])\n",
    "    #        print(Q_value)\n",
    "            return self.chosenAction\n",
    "        else:\n",
    "            print('Not on Ground')\n",
    "            return self.chosenAction\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_epsilon * episode)\n",
    "\n",
    "    # Train the network\n",
    "    def train(self, batch_size):\n",
    "        # minibatch from memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Get variables from batch so we can find q-value\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.main_network.predict(state)\n",
    "            print(target)\n",
    "            \n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                target[0][action] = (reward + self.gamma * np.amax(self.target_network.predict(next_state)))\n",
    "                \n",
    "            self. main_network.fit(state, target, epochs=1, verbose=0)    \n",
    "        \n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def get_pred_act(self, state):\n",
    "        Q_values = self.main_network.predict(state)\n",
    "        return np.argmax(Q_values[0])\n",
    "        \n",
    "        \n",
    "    def load(self, name):\n",
    "        self.main_network = load_model(name)\n",
    "        self.target_network = load_model(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        save_model(self.main_network, name)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e976bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = (80, 88, 1)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_state(state):\n",
    "    image = Image.fromarray(state)\n",
    "    image = image.resize((88,80))\n",
    "    image = image.convert('L')\n",
    "    image = np.array(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "#dqn = DQNAgent(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61c8e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000000\n",
    "num_timesteps = 400000\n",
    "batch_size = 64\n",
    "DEBUG_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9190842",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(state_space, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d94f649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.199637  7.646951  8.630537 10.829005  5.00468 ]]\n",
      "[[ 7.106808   7.4788017  8.346444  10.763955   4.8562636]]\n",
      "[[ 7.3428607  7.709157   8.604187  11.073244   4.8742123]]\n",
      "[[ 7.3934216  7.680994   8.351464  11.06198    4.8374977]]\n",
      "[[ 7.46939    7.93691    8.4656725 11.39269    5.290879 ]]\n",
      "[[ 7.3940897  7.73989    8.407948  11.301067   5.318516 ]]\n",
      "[[ 7.5002165  8.151784   8.496263  11.388604   5.8393445]]\n",
      "[[6.2925725 6.482702  7.2342052 8.744157  4.3715534]]\n",
      "[[ 7.495154   8.405509   8.3261385 10.71694    6.099004 ]]\n",
      "[[ 7.368479  8.241348  8.383222 10.073493  5.952163]]\n",
      "[[ 7.61528   8.714278  8.883165 10.364555  6.406973]]\n",
      "[[ 7.963313   9.427551   9.184759  10.9830675  7.3127728]]\n",
      "[[ 8.322979  9.959121  9.550048 11.183495  8.023121]]\n",
      "[[ 8.169033   9.7305155  9.655089  10.551677   8.081961 ]]\n",
      "[[ 8.618447 10.629013 10.075827 11.229962  9.122628]]\n",
      "[[ 9.025598  11.010551  10.489175  11.2540865  9.79328  ]]\n",
      "[[ 8.605777 10.660883 10.031108 10.392437  9.657513]]\n",
      "[[ 8.700758 10.684738 10.189766 10.065006 10.004009]]\n",
      "[[8.11301  9.935741 9.608131 8.827196 9.434792]]\n",
      "[[6.9495745 8.249724  8.364108  6.7750936 7.826644 ]]\n",
      "[[7.5283856 9.192869  9.072905  7.5071163 8.827878 ]]\n",
      "[[7.2204213 8.671371  8.467778  6.9364114 8.302244 ]]\n",
      "[[5.7484365 6.570246  7.0052185 4.716082  6.190467 ]]\n",
      "[[5.4343314 6.135695  6.627136  4.2372613 5.7897387]]\n",
      "[[6.4820294 7.6839314 7.6310453 5.539546  7.3157964]]\n",
      "[[5.87862   6.767157  7.1157403 4.6690855 6.4536633]]\n",
      "[[6.2007604 7.241986  7.3320184 4.866201  6.9404516]]\n",
      "[[6.4065833 7.3413663 7.508321  4.8966923 7.0019755]]\n",
      "[[5.2977653 5.723625  6.58738   3.1826832 5.527891 ]]\n",
      "[[6.886422  7.730445  8.025225  5.0140915 7.3575106]]\n",
      "[[7.6590853 8.667008  9.118624  5.818138  8.232049 ]]\n",
      "[[8.128108 9.244881 9.587383 6.305172 8.754255]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-3badd2ea2021>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-38464a99223b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mmain_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     dataset = dataset.map(\n\u001b[1;32m--> 397\u001b[1;33m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1700\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1702\u001b[1;33m           preserve_cardinality=True)\n\u001b[0m\u001b[0;32m   1703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4100\u001b[0m         \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4101\u001b[0m         \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4102\u001b[1;33m         **self._flat_structure)\n\u001b[0m\u001b[0;32m   4103\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParallelMapDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, name)\u001b[0m\n\u001b[0;32m   5087\u001b[0m         \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5088\u001b[0m         \u001b[1;34m\"use_inter_op_parallelism\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"deterministic\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5089\u001b[1;33m         deterministic, \"preserve_cardinality\", preserve_cardinality)\n\u001b[0m\u001b[0;32m   5090\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5091\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('STARTING TRAINING')\n",
    "\n",
    "stuck_buffer = deque(maxlen=DEBUG_LENGTH)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    Return = 0\n",
    "    done = False\n",
    "    time_step = 0\n",
    "    onGround = 79\n",
    "    \n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80, 88, 1)\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        env.render()\n",
    "        time_step += 1\n",
    "        \n",
    "        if t > 1 and stuck_buffer.count(stuck_buffer[-1]) > DEBUG_LENGTH - 50:\n",
    "            action = dqn.act(state, onGround = 79)\n",
    "        else:\n",
    "            action = dqn.act(state, onGround)\n",
    "            \n",
    "        \n",
    "        print('ACTION IS ' + str(action)) \n",
    "        \n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #print(info['y_pos'])\n",
    "        onGround = info['y_pos']\n",
    "        stuck_buffer.append(info['x_pos'])\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80, 88, 1)\n",
    "        \n",
    "        dqn.store_transition(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        Return += reward\n",
    "        print('Episode is: {}\\nTotal Time Step: {}\\nCurrent Reward: {}\\nEpsilon is: {}'.format(str(i), str(time_step), str(Return), str(dqn.epsilon)))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        if len(dqn.memory) > batch_size and i > 5:\n",
    "            dqn.train(batch_size)\n",
    "            \n",
    "    dqn.update_epsilon(i)\n",
    "    clear_output(wait=True)\n",
    "    dqn.update_target_network()\n",
    "    # Save Model\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ef89025",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save('marioRL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96a9c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load('MarioRL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05a77e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not on Ground\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-2c462928e023>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0monGround\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m88\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-7a472214b048>\u001b[0m in \u001b[0;36mpreprocess_state\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m88\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1941\u001b[0m                 )\n\u001b[0;32m   1942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1943\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1945\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Visualizing model\n",
    "\n",
    "while 1:\n",
    "    done = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80, 88, 1)\n",
    "    total_reward = 0\n",
    "    onGround = 79\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = dqn.act(state, onGround)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        onGround = info['y_pos']\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80, 88, 1)\n",
    "        state = next_state\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377473f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
