{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea662a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing mario dependencies\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import save_model\n",
    "from keras.models import load_model \n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e3b44b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym_super_mario_bros' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6f3c1d50bcbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym_super_mario_bros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SuperMarioBros-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJoypadSpace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRIGHT_ONLY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym_super_mario_bros' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a41947",
   "metadata": {},
   "source": [
    "total_reward = 0\n",
    "done = True\n",
    "\n",
    "for step in range(100000):\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    print(info)\n",
    "    total_reward += reward\n",
    "    clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c2ffdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Create variables for agent\n",
    "        self.state_space = state_size\n",
    "        self.action_space = action_size\n",
    "        self.memory = deque(maxlen = 5000)\n",
    "        self.gamma = 0.8\n",
    "        self.chosenAction = 0\n",
    "        \n",
    "        # Exploration vs explotation\n",
    "        self.epsilon = 1\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay_epsilon = 0.0001\n",
    "        \n",
    "        # Building Neural Networks for Agent\n",
    "        self.main_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (4,4), strides=4, padding='same', input_shape=self.state_space))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (4,4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (3,3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())  \n",
    "        \n",
    "    def act(self, state, onGround):\n",
    "        if onGround < 83:\n",
    "            print('On Ground')\n",
    "            if random.uniform(0,1) < self.epsilon:\n",
    "                self.chosenAction = np.random.randint(self.action_space)\n",
    "                return self.chosenAction\n",
    "            Q_value = self.main_network.predict(state)\n",
    "            self.chosenAction = np.argmax(Q_value[0])\n",
    "    #        print(Q_value)\n",
    "            return self.chosenAction\n",
    "        else:\n",
    "            print('Not on Ground')\n",
    "            return self.chosenAction\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_epsilon * episode)\n",
    "\n",
    "    # Train the network\n",
    "    def train(self, batch_size):\n",
    "        # minibatch from memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Get variables from batch so we can find q-value\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.main_network.predict(state)\n",
    "            print(target)\n",
    "            \n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                target[0][action] = (reward + self.gamma * np.amax(self.target_network.predict(next_state)))\n",
    "                \n",
    "            self. main_network.fit(state, target, epochs=1, verbose=0)    \n",
    "        \n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def get_pred_act(self, state):\n",
    "        Q_values = self.main_network.predict(state)\n",
    "        return np.argmax(Q_values[0])\n",
    "        \n",
    "        \n",
    "    def load(self, name):\n",
    "        self.main_network = load_model(name)\n",
    "        self.target_network = load_model(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        save_model(self.main_network, name)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e976bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = (80, 88, 1)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_state(state):\n",
    "    image = Image.fromarray(state)\n",
    "    image = image.resize((88,80))\n",
    "    image = image.convert('L')\n",
    "    image = np.array(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "#dqn = DQNAgent(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61c8e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000000\n",
    "num_timesteps = 400000\n",
    "batch_size = 64\n",
    "DEBUG_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9190842",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(state_space, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcb752a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load('MarioRL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d94f649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.2937758 23.531214  13.650911  33.302364  24.283241 ]]\n",
      "[[ 4.3187737 13.163657   9.982404  19.301273   6.511949 ]]\n",
      "[[ 5.7367096 10.284222   9.135763  15.296214   2.6382604]]\n",
      "[[ 6.12954   10.53536    9.196941  15.60462    2.9019296]]\n",
      "[[11.982199  15.12371   13.132543  22.341642   3.6461887]]\n",
      "[[18.729708 18.463772 15.879629 27.140587  3.842517]]\n",
      "[[16.527405    6.4254904  10.737882   14.334472    0.83452004]]\n",
      "[[14.602374    0.15833834  8.981685    6.365984   -2.2022147 ]]\n",
      "[[14.81761    -0.72502446 11.588421    5.9502664  -1.640629  ]]\n",
      "[[9.277575  1.3161894 8.731137  5.9944057 2.4218855]]\n",
      "[[7.5541615 2.806184  7.222246  6.229103  2.8733866]]\n",
      "[[7.79278  4.495775 7.303487 7.700537 4.015381]]\n",
      "[[11.756711   7.2552543 11.360169  11.809618   8.232837 ]]\n",
      "[[12.453796  8.773743 12.582499 12.007861 10.843824]]\n",
      "[[13.955826 10.49309  15.153606 12.487238 15.023069]]\n",
      "[[11.462698  9.091023 12.648635  9.653771 13.10728 ]]\n",
      "[[ 9.658142   7.981949  10.815672   7.6566887 11.184681 ]]\n",
      "[[ 9.304737   7.8509817 11.387096   6.517487  11.051649 ]]\n",
      "[[10.617525   8.479135  12.93307    6.2772927 11.8973255]]\n",
      "[[12.411219  10.678727  15.013248   6.4401045 13.323466 ]]\n",
      "[[12.819486  11.530881  14.280931   6.3311195 13.285488 ]]\n",
      "[[12.809589  11.8213825 12.924336   6.258601  12.752545 ]]\n",
      "[[13.765206 13.416641 13.076267  6.432207 13.365171]]\n",
      "[[13.462363 13.791879 11.923568  6.30698  12.678373]]\n",
      "[[12.47939   13.215621  10.451279   5.8718615 10.883246 ]]\n",
      "[[12.986412  14.1109705 10.447373   6.0141473 11.226125 ]]\n",
      "[[13.741834 15.214213 11.007541  6.152961 11.840827]]\n",
      "[[14.360447  16.177372  11.554531   6.2990885 12.388609 ]]\n",
      "[[15.282904 17.52656  12.198728  6.554984 12.952298]]\n",
      "[[15.254222 17.470505 11.733403  6.397686 12.678577]]\n",
      "[[16.304878 18.793566 12.168813  6.812014 13.54047 ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-549b366c99fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-38464a99223b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mmain_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1593\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1594\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1595\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1596\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1139\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[0;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m    721\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_job_token\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         gen_experimental_dataset_ops.make_data_service_iterator(\n",
      "\u001b[1;32mc:\\users\\kamuela\\anaconda3\\envs\\practicalrl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3005\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m   3006\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MakeIterator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3007\u001b[1;33m         tld.op_callbacks, dataset, iterator)\n\u001b[0m\u001b[0;32m   3008\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3009\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('STARTING TRAINING')\n",
    "\n",
    "stuck_buffer = deque(maxlen=DEBUG_LENGTH)\n",
    "y_buffer = deque(maxlen=DEBUG_LENGTH)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    Return = 0\n",
    "    done = False\n",
    "    time_step = 0\n",
    "    onGround = 79\n",
    "    \n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80, 88, 1)\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        env.render()\n",
    "        time_step += 1\n",
    "        \n",
    "        if t > 1 and stuck_buffer.count(stuck_buffer[-1]) > DEBUG_LENGTH - 50:\n",
    "            action = dqn.act(state, onGround = 79)\n",
    "        else:\n",
    "            action = dqn.act(state, onGround)\n",
    "            \n",
    "    \n",
    "            \n",
    "        \n",
    "        print('ACTION IS ' + str(action)) \n",
    "        \n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #print(info['y_pos'])\n",
    "        onGround = info['y_pos']\n",
    "        stuck_buffer.append(info['x_pos'])\n",
    "        y_buffer.append(info['y_pos'])\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80, 88, 1)\n",
    "        \n",
    "        dqn.store_transition(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        Return += reward\n",
    "        print('Episode is: {}\\nTotal Time Step: {}\\nCurrent Reward: {}\\nEpsilon is: {}'.format(str(i), str(time_step), str(Return), str(dqn.epsilon)))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        if len(dqn.memory) > batch_size and i > 5:\n",
    "            dqn.train(batch_size)\n",
    "            \n",
    "    dqn.update_epsilon(i)\n",
    "    clear_output(wait=True)\n",
    "    dqn.update_target_network()\n",
    "    # Save Model\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ef89025",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save('marioRL.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb523580",
   "metadata": {},
   "source": [
    "dqn.load('MarioRL.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aca029",
   "metadata": {},
   "source": [
    "# Visualizing model\n",
    "\n",
    "while 1:\n",
    "    done = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80, 88, 1)\n",
    "    total_reward = 0\n",
    "    onGround = 79\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = dqn.act(state, onGround)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        onGround = info['y_pos']\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80, 88, 1)\n",
    "        state = next_state\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377473f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
